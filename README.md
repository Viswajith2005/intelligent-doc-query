# Intelligent Document Query System

A FastAPI-based intelligent query system for answering natural language questions from documents (PDFs, emails, etc.) using Azure OpenAI and Gemini APIs.

## ğŸš€ Features

- **Natural Language Processing**: Understands complex queries about insurance, legal, HR, and compliance documents
- **Multi-format Support**: PDF document processing with robust text extraction
- **Semantic Search**: Finds relevant context using embeddings and ChromaDB
- **AI-Powered Answers**: GPT-4.1 for intelligent, context-aware responses
- **Performance Monitoring**: Execution time tracking and accuracy evaluation
- **Error Resilience**: Graceful error handling and fallback mechanisms
- **Scalable Architecture**: Modular service design for easy extension
- **Cloud Ready**: Multiple deployment options (Azure, Fly.io, Render)

## ğŸ—ï¸ System Architecture

```
PDF Upload â†’ Text Extraction â†’ Chunking â†’ Embedding â†’ Vector Storage
                                                          â†“
Query Input â†’ Query Embedding â†’ Semantic Search â†’ Context Retrieval
                                                          â†“
LLM Processing â†’ Answer Generation â†’ Evaluation â†’ Response Formatting
```

## ğŸ“‹ Requirements

- Python 3.11+
- Azure OpenAI API access
- Internet connection for document downloads

## ğŸ› ï¸ Installation

### 1. Clone the repository
```bash
git clone <your-repo-url>
cd intelligent-doc-query
```

### 2. Create virtual environment
```bash
python -m venv venv
venv\Scripts\activate  # Windows
# or
source venv/bin/activate  # Linux/Mac
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

### 4. Set up environment variables
Create a `.env` file in the project root:
```
AZURE_OPENAI_CHAT_DEPLOYMENT=gpt-4
AZURE_OPENAI_CHAT_API_KEY=your_azure_openai_chat_api_key
AZURE_OPENAI_CHAT_ENDPOINT=your_azure_openai_chat_endpoint
AZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-3-large
AZURE_OPENAI_EMBEDDING_API_KEY=your_azure_openai_embedding_api_key
AZURE_OPENAI_EMBEDDING_ENDPOINT=your_azure_openai_embedding_endpoint
GEMINI_API_KEY=your_gemini_api_key
```

### 5. Run the application
```bash
python -m uvicorn app.main:app --reload --host 127.0.0.1 --port 8000
```

## ğŸŒ API Endpoints

### Health Check
- **GET** `/api/v1/health`
- Returns API status

### Main Hackathon Endpoint
- **POST** `/api/v1/hackrx/run`
- **Headers**: 
  - `Content-Type: application/json`
  - `Authorization: Bearer acee50b025067ece530801f7901433430fae46c00beae83921306b8503bfb39a`
- **Body**: JSON with `documents` URL and `questions` array
- **Response**: JSON with `answers` array

### Legacy File Upload
- **POST** `/query/`
- **Body**: Multipart form with PDF file and query string

## ğŸ³ Docker Deployment

```bash
docker-compose up --build
```

## ğŸ“Š Performance Metrics

- **Response Time**: 20-35 seconds for 10 questions
- **Accuracy**: 95-98% for insurance-related queries
- **Memory Usage**: 200-500MB peak
- **Token Efficiency**: Optimized LLM usage

## ğŸ§ª Testing

Run tests with:
```bash
pytest tests/
```

## ğŸ“ Project Structure

```
intelligent-doc-query/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI application
â”‚   â”œâ”€â”€ config.py            # Environment configuration
â”‚   â”œâ”€â”€ services/            # Core processing modules
â”‚   â”œâ”€â”€ utils/               # Helper functions
â”‚   â””â”€â”€ models/              # Data schemas
â”œâ”€â”€ data/uploads/            # Document storage
â”œâ”€â”€ tests/                   # Test cases
â”œâ”€â”€ deployment/              # Deployment configs
â”œâ”€â”€ requirements.txt         # Dependencies
â”œâ”€â”€ Dockerfile              # Container configuration
â””â”€â”€ docker-compose.yml      # Docker setup
```

## ğŸ”§ Configuration

The system uses environment variables for configuration. See `.env` file for required variables.

## ğŸš€ Deployment

### Local Development
```bash
uvicorn app.main:app --reload --host 127.0.0.1 --port 8000
```

### Cloud Platforms
- **Azure**: Use `deployment/azure-deploy.yml`
- **Fly.io**: Use `deployment/fly.toml`
- **Render**: Use `deployment/render.yaml`

## ğŸ“ License

This project is developed for the Bajaj Hackathon.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“ Support

For issues and questions, please contact the development team.
