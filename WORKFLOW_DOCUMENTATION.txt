===============================================================================
                    INTELLIGENT DOCUMENT QUERY SYSTEM
                           Complete Workflow Documentation
===============================================================================

PROJECT OVERVIEW:
This is a FastAPI-based intelligent document query system that allows users to ask 
natural language questions about uploaded documents (primarily PDFs) and get AI-powered 
answers. The system uses Azure OpenAI GPT-4.1 and Google Gemini for LLM processing, 
with ChromaDB for vector storage and semantic search.

===============================================================================
                            SYSTEM ARCHITECTURE
===============================================================================

ðŸ“ PROJECT STRUCTURE:
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py (FastAPI application - Main API endpoints)
â”‚   â”œâ”€â”€ config.py (Environment configuration - API keys, endpoints)
â”‚   â”œâ”€â”€ services/ (Core processing modules)
â”‚   â”‚   â”œâ”€â”€ document_loader.py (PDF text extraction)
â”‚   â”‚   â”œâ”€â”€ chunker.py (Document chunking)
â”‚   â”‚   â”œâ”€â”€ embedder.py (Text to vector conversion)
â”‚   â”‚   â”œâ”€â”€ vector_store.py (ChromaDB operations)
â”‚   â”‚   â”œâ”€â”€ llm_service.py (GPT-4.1 integration)
â”‚   â”‚   â””â”€â”€ evaluator.py (Response evaluation)
â”‚   â”œâ”€â”€ utils/ (Helper functions)
â”‚   â”‚   â””â”€â”€ helpers.py (File management, response formatting)
â”‚   â””â”€â”€ models/ (Data schemas)
â”‚       â””â”€â”€ schemas.py (Pydantic models)
â”œâ”€â”€ data/uploads/ (Document storage)
â”œâ”€â”€ tests/ (Test cases)
â””â”€â”€ deployment/ (Deployment configs for Azure, Fly.io, Render)

===============================================================================
                            COMPLETE WORKFLOW BREAKDOWN
===============================================================================

1. SYSTEM INITIALIZATION
------------------------
â€¢ Application starts with FastAPI server
â€¢ Configuration loads from environment variables:
  - Azure OpenAI API keys and endpoints
  - Gemini API key
  - Model deployment names
â€¢ FileManager initializes upload directory
â€¢ ChromaDB client initializes for vector storage

2. API ENDPOINT HANDLING
-------------------------
ENDPOINT: POST /query/
INPUT: 
  - file: UploadFile (PDF document)
  - query: str (Natural language question)

PROCESS:
â€¢ Receives PDF file via FastAPI's UploadFile
â€¢ Accepts natural language query string
â€¢ Records execution start time for performance tracking
â€¢ Validates file format and size

3. DOCUMENT PROCESSING PIPELINE
--------------------------------

STEP 3.1: FILE MANAGEMENT (app/utils/helpers.py)
â€¢ FileManager.save_uploaded_file():
  - Generates unique UUID for file
  - Saves uploaded file to ./data/uploads/ directory
  - Returns file path for processing
â€¢ FileManager.generate_document_id():
  - Creates unique document ID for tracking
  - Uses filename + content hash for consistency

STEP 3.2: DOCUMENT LOADING (app/services/document_loader.py)
â€¢ load_document(file_path: str) -> str:
  - Uses PyMuPDF (fitz) to extract text from PDF
  - Iterates through all pages
  - Concatenates text content
  - Returns cleaned text string
  - Handles PDF parsing errors gracefully

STEP 3.3: DOCUMENT CHUNKING (app/services/chunker.py)
â€¢ chunk_document(document: str, max_chunk_size: int = 500) -> list:
  - Splits document into words
  - Creates chunks of 500 words each
  - Ensures manageable pieces for embedding
  - Returns list of text chunks
  - Maintains semantic coherence within chunks

STEP 3.4: EMBEDDING GENERATION (app/services/embedder.py)
â€¢ embed_chunks(chunks):
  - Uses Azure OpenAI text-embedding-3-large model
  - Converts text chunks to vector embeddings
  - Each embedding represents semantic meaning of chunk
  - Returns list of embedding vectors (1536 dimensions)
  - Handles API rate limiting and errors

STEP 3.5: VECTOR STORAGE (app/services/vector_store.py)
â€¢ store_embeddings(doc_id: str, chunks: list[str], embeddings: list[list[float]]):
  - Uses ChromaDB for vector database
  - Creates unique collection per document
  - Stores chunks with their embeddings
  - Indexes for fast similarity search
  - Persists data to .chromadb directory

4. QUERY PROCESSING PIPELINE
-----------------------------

STEP 4.1: SEMANTIC SEARCH (app/services/vector_store.py)
â€¢ search_similar_chunks(query: str, doc_id: str, top_k: int = 5) -> list[str]:
  - Embeds the user query using same model
  - Performs similarity search in ChromaDB
  - Returns top 5 most relevant chunks
  - Uses cosine similarity for matching
  - Handles cases where document not found

STEP 4.2: LLM QUERYING (app/services/llm_service.py)
â€¢ query_llm(prompt, context_chunks):
  - Uses Azure OpenAI GPT-4.1 model
  - Combines user query with relevant context chunks
  - Formats prompt: "{query}\n\nContext:\n{relevant_chunks}"
  - Generates natural language answer
  - Handles connection errors gracefully
  - Uses stable API version (2024-02-15-preview)

STEP 4.3: RESPONSE EVALUATION (app/services/evaluator.py)
â€¢ evaluate_response(query: str, answer: str) -> str:
  - Analyzes answer for insurance-related keywords
  - Categorizes as "Likely Yes", "Likely No", or "Uncertain"
  - Uses simple keyword matching for classification
  - Helps users quickly understand coverage status

5. RESPONSE FORMATTING (app/utils/helpers.py)
---------------------------------------------
â€¢ ResponseFormatter.format_success_response():
  - Structures response in consistent format
  - Includes status, message, and data
  - Calculates execution time
  - Returns JSON response

RESPONSE STRUCTURE:
{
  "status": "success",
  "message": "Success",
  "data": {
    "query": "Does it cover knee surgery?",
    "answer": "Based on the document...",
    "evaluation": "Likely Yes",
    "execution_time_seconds": 3.45
  }
}

6. ERROR HANDLING & CLEANUP
----------------------------
ERROR HANDLING:
â€¢ Try-catch blocks around each processing step
â€¢ HTTP 500 errors for internal server issues
â€¢ Graceful degradation for API failures
â€¢ Detailed error logging with traceback
â€¢ User-friendly error messages

CLEANUP PROCESS:
â€¢ Automatic file deletion after processing
â€¢ Memory cleanup for temporary data
â€¢ ChromaDB persistence for vector storage
â€¢ Async cleanup operations

===============================================================================
                            TECHNICAL ARCHITECTURE
===============================================================================

DEPENDENCIES & TECHNOLOGIES:
â€¢ FastAPI: Web framework for API
â€¢ PyMuPDF: PDF text extraction
â€¢ Azure OpenAI: GPT-4.1 for chat, text-embedding-3-large for embeddings
â€¢ ChromaDB: Vector database for semantic search
â€¢ Uvicorn: ASGI server
â€¢ Docker: Containerization
â€¢ Pytest: Testing framework
â€¢ Python-dotenv: Environment management

DATA FLOW:
PDF Upload â†’ Text Extraction â†’ Chunking â†’ Embedding â†’ Vector Storage
                                                          â†“
Query Input â†’ Query Embedding â†’ Semantic Search â†’ Context Retrieval
                                                          â†“
LLM Processing â†’ Answer Generation â†’ Evaluation â†’ Response Formatting

PERFORMANCE OPTIMIZATIONS:
â€¢ Async file handling for non-blocking operations
â€¢ Chunk-based processing for large documents
â€¢ Vector similarity search for fast retrieval
â€¢ Execution time tracking for monitoring
â€¢ Automatic cleanup to prevent memory leaks
â€¢ Connection pooling for API calls

===============================================================================
                            DEPLOYMENT OPTIONS
===============================================================================

LOCAL DEVELOPMENT:
```bash
uvicorn app.main:app --reload --host 127.0.0.1 --port 8000
```

DOCKER DEPLOYMENT:
```bash
docker-compose up --build
```

CLOUD PLATFORMS:
â€¢ Azure: deployment/azure-deploy.yml
â€¢ Fly.io: deployment/fly.toml
â€¢ Render: deployment/render.yaml

===============================================================================
                            TESTING FRAMEWORK
===============================================================================

API TESTS (tests/test_api.py):
â€¢ Health check endpoint testing
â€¢ Query endpoint validation
â€¢ Error handling verification
â€¢ File upload testing

SERVICE TESTS (tests/test_services.py):
â€¢ Document chunking validation
â€¢ Embedding generation testing
â€¢ Vector store operations verification
â€¢ LLM integration testing

===============================================================================
                            KEY FEATURES
===============================================================================

1. NATURAL LANGUAGE PROCESSING
   - Understands complex queries
   - Handles insurance-specific terminology
   - Context-aware responses

2. MULTI-FORMAT SUPPORT
   - PDF document processing
   - Extensible for other formats
   - Robust text extraction

3. SEMANTIC SEARCH
   - Finds relevant context using embeddings
   - Cosine similarity matching
   - Top-k retrieval for accuracy

4. AI-POWERED ANSWERS
   - GPT-4.1 for intelligent responses
   - Context-aware generation
   - Natural language output

5. PERFORMANCE MONITORING
   - Execution time tracking
   - Response time optimization
   - Performance metrics

6. ERROR RESILIENCE
   - Graceful error handling
   - Fallback mechanisms
   - Detailed error logging

7. SCALABLE ARCHITECTURE
   - Modular service design
   - Microservices pattern
   - Easy to extend and maintain

8. CLOUD READY
   - Multiple deployment options
   - Containerized application
   - Environment-based configuration

===============================================================================
                            USE CASE EXAMPLE
===============================================================================

INPUT:
â€¢ File: insurance_policy.pdf
â€¢ Query: "Does this policy cover knee surgery?"

WORKFLOW:
1. PDF uploaded and saved temporarily to ./data/uploads/
2. Text extracted from all pages using PyMuPDF
3. Document chunked into 500-word pieces
4. Each chunk embedded into 1536-dimensional vectors
5. Vectors stored in ChromaDB with document ID
6. Query "knee surgery" embedded using same model
7. Similar chunks retrieved (top 5 most relevant)
8. Context + query sent to GPT-4.1 with prompt formatting
9. AI generates answer based on policy content
10. Answer evaluated for coverage likelihood
11. Response formatted with execution time
12. Temporary files cleaned up automatically

OUTPUT:
{
  "status": "success",
  "message": "Success",
  "data": {
    "query": "Does this policy cover knee surgery?",
    "answer": "Yes, knee surgery is covered under Section 3.2 of your policy. The policy specifically mentions orthopedic procedures including knee surgery with a coverage limit of $50,000 per procedure.",
    "evaluation": "Likely Yes",
    "execution_time_seconds": 4.2
  }
}

===============================================================================
                            API ENDPOINTS
===============================================================================

GET / - Health check endpoint
â€¢ Returns welcome message
â€¢ Redirects to /docs for API documentation

POST /query/ - Main query endpoint
â€¢ Accepts: file (PDF), query (string)
â€¢ Returns: structured response with answer and evaluation
â€¢ Handles file upload and processing
â€¢ Includes execution time tracking

===============================================================================
                            ENVIRONMENT VARIABLES
===============================================================================

Required Environment Variables:
â€¢ AZURE_OPENAI_CHAT_DEPLOYMENT: GPT-4 deployment name
â€¢ AZURE_OPENAI_CHAT_API_KEY: Azure OpenAI API key
â€¢ AZURE_OPENAI_CHAT_ENDPOINT: Azure OpenAI endpoint
â€¢ AZURE_OPENAI_EMBEDDING_DEPLOYMENT: text-embedding-3-large deployment
â€¢ AZURE_OPENAI_EMBEDDING_API_KEY: Embedding API key
â€¢ AZURE_OPENAI_EMBEDDING_ENDPOINT: Embedding endpoint
â€¢ GEMINI_API_KEY: Google Gemini API key (optional)

===============================================================================
                            PERFORMANCE CHARACTERISTICS
===============================================================================

TYPICAL EXECUTION TIMES:
â€¢ File upload: 0.1-0.5 seconds
â€¢ Text extraction: 0.2-1.0 seconds
â€¢ Chunking: 0.1 seconds
â€¢ Embedding generation: 1-3 seconds
â€¢ Vector storage: 0.5 seconds
â€¢ Semantic search: 0.2-0.5 seconds
â€¢ LLM processing: 2-5 seconds
â€¢ Total response time: 4-10 seconds

MEMORY USAGE:
â€¢ Document processing: 50-200MB depending on PDF size
â€¢ Vector storage: 10-50MB per document
â€¢ LLM context: 1-5MB per query
â€¢ Peak memory: 200-500MB for large documents

===============================================================================
                            SECURITY CONSIDERATIONS
===============================================================================

â€¢ API key management through environment variables
â€¢ Temporary file cleanup after processing
â€¢ Input validation for file types and sizes
â€¢ Error messages don't expose internal details
â€¢ Secure file handling with unique IDs
â€¢ Rate limiting considerations for API calls

===============================================================================
                            FUTURE ENHANCEMENTS
===============================================================================

POTENTIAL IMPROVEMENTS:
â€¢ Support for more document formats (DOCX, TXT)
â€¢ User authentication and session management
â€¢ Document versioning and history
â€¢ Advanced evaluation metrics
â€¢ Multi-language support
â€¢ Real-time processing status updates
â€¢ Batch processing capabilities
â€¢ Advanced caching mechanisms

===============================================================================
                            CONCLUSION
===============================================================================

This Intelligent Document Query System provides a complete end-to-end solution for 
intelligent document querying with enterprise-grade reliability and performance. 
The modular architecture makes it easy to extend, maintain, and deploy across 
different platforms while providing fast, accurate responses to natural language 
queries about document content.

The system successfully combines modern AI technologies (GPT-4.1, embeddings) 
with robust document processing (PyMuPDF, ChromaDB) to create a powerful tool 
for extracting insights from documents through natural language interaction.

=============================================================================== 